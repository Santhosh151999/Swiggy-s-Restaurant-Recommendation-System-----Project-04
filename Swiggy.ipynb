{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aef05f4-babe-4e04-8103-09f4c5bd64f3",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c446ddf1-4f99-4406-9144-8b56d91ed3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9084/1008320741.py:16: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df['rating_count'] = df['rating_count'].str.extract('(\\d+)')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>cost</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>567335</td>\n",
       "      <td>AB FOODS POINT</td>\n",
       "      <td>Abohar</td>\n",
       "      <td>3.894461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>Beverages,Pizzas</td>\n",
       "      <td>AB FOODS POINT, NEAR RISHI NARANG DENTAL CLINI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>531342</td>\n",
       "      <td>Janta Sweet House</td>\n",
       "      <td>Abohar</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>Sweets,Bakery</td>\n",
       "      <td>Janta Sweet House, Bazar No.9, Circullar Road,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>158203</td>\n",
       "      <td>theka coffee desi</td>\n",
       "      <td>Abohar</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>theka coffee desi, sahtiya sadan road city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187912</td>\n",
       "      <td>Singh Hut</td>\n",
       "      <td>Abohar</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Fast Food,Indian</td>\n",
       "      <td>Singh Hut, CIRCULAR ROAD NEAR NEHRU PARK ABOHAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>543530</td>\n",
       "      <td>GRILL MASTERS</td>\n",
       "      <td>Abohar</td>\n",
       "      <td>3.894461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Italian-American,Fast Food</td>\n",
       "      <td>GRILL MASTERS, ADA Heights, Abohar - Hanumanga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id               name    city    rating  rating_count   cost  \\\n",
       "0  567335     AB FOODS POINT  Abohar  3.894461           0.0  200.0   \n",
       "1  531342  Janta Sweet House  Abohar  4.400000          50.0  200.0   \n",
       "2  158203  theka coffee desi  Abohar  3.800000         100.0  100.0   \n",
       "3  187912          Singh Hut  Abohar  3.700000          20.0  250.0   \n",
       "4  543530      GRILL MASTERS  Abohar  3.894461           0.0  250.0   \n",
       "\n",
       "                      cuisine  \\\n",
       "0            Beverages,Pizzas   \n",
       "1               Sweets,Bakery   \n",
       "2                   Beverages   \n",
       "3            Fast Food,Indian   \n",
       "4  Italian-American,Fast Food   \n",
       "\n",
       "                                             address  \n",
       "0  AB FOODS POINT, NEAR RISHI NARANG DENTAL CLINI...  \n",
       "1  Janta Sweet House, Bazar No.9, Circullar Road,...  \n",
       "2         theka coffee desi, sahtiya sadan road city  \n",
       "3    Singh Hut, CIRCULAR ROAD NEAR NEHRU PARK ABOHAR  \n",
       "4  GRILL MASTERS, ADA Heights, Abohar - Hanumanga...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load original dataset\n",
    "df = pd.read_csv(\"swiggy.csv\")\n",
    "\n",
    "# Step 2: Drop duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Step 3: Clean and impute 'rating'\n",
    "df['rating'] = df['rating'].replace('--', np.nan)\n",
    "df['rating'] = df['rating'].astype(float)\n",
    "df['rating'] = df['rating'].fillna(df['rating'].mean())  # Fill with mean rating\n",
    "\n",
    "# Step 4: Clean and impute 'rating_count'\n",
    "df['rating_count'] = df['rating_count'].str.extract('(\\d+)')\n",
    "df['rating_count'] = df['rating_count'].astype(float)\n",
    "df['rating_count'] = df['rating_count'].fillna(0)  # Fill missing with 0\n",
    "\n",
    "# Step 5: Clean and impute 'cost'\n",
    "df['cost'] = df['cost'].replace('‚Çπ', '', regex=True).str.strip()\n",
    "df['cost'] = df['cost'].astype(float)\n",
    "df['cost'] = df['cost'].fillna(df['cost'].median())  # Fill missing with median cost\n",
    "\n",
    "# Step 6: Drop rows missing in important categorical columns\n",
    "df = df.dropna(subset=['name', 'city', 'cuisine'])\n",
    "\n",
    "# Step 7: Drop unneeded columns\n",
    "df = df.drop(columns=['lic_no', 'link', 'menu'])\n",
    "\n",
    "# Step 8: Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Step 9: Save cleaned data\n",
    "df.to_csv(\"cleaned_data.csv\", index=False)\n",
    "\n",
    "# Step 10: Preview\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132662ed-c334-4c12-914b-94fa52745900",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07e3100-ab9a-4a8a-be44-ca703c45f0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>cost</th>\n",
       "      <th>city_Abids &amp; Koti,Hyderabad</th>\n",
       "      <th>city_Abohar</th>\n",
       "      <th>city_Adajan,Surat</th>\n",
       "      <th>city_Adilabad</th>\n",
       "      <th>city_Adityapur</th>\n",
       "      <th>city_Adoni</th>\n",
       "      <th>city_Adyar,Chennai</th>\n",
       "      <th>...</th>\n",
       "      <th>cuisine_Thalis</th>\n",
       "      <th>cuisine_Tibetan</th>\n",
       "      <th>cuisine_Tribal</th>\n",
       "      <th>cuisine_Turkish</th>\n",
       "      <th>cuisine_Use Code JUMBO30 to avail</th>\n",
       "      <th>cuisine_Use code XPRESS121 to avail.</th>\n",
       "      <th>cuisine_Vietnamese</th>\n",
       "      <th>cuisine_Waffle</th>\n",
       "      <th>cuisine_indian</th>\n",
       "      <th>original_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.894461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.894461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.400000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.400000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 951 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating  rating_count   cost  city_Abids & Koti,Hyderabad  city_Abohar  \\\n",
       "0  3.894461           0.0  200.0                          0.0          1.0   \n",
       "1  3.894461           0.0  200.0                          0.0          1.0   \n",
       "2  4.400000          50.0  200.0                          0.0          1.0   \n",
       "3  4.400000          50.0  200.0                          0.0          1.0   \n",
       "4  3.800000         100.0  100.0                          0.0          1.0   \n",
       "\n",
       "   city_Adajan,Surat  city_Adilabad  city_Adityapur  city_Adoni  \\\n",
       "0                0.0            0.0             0.0         0.0   \n",
       "1                0.0            0.0             0.0         0.0   \n",
       "2                0.0            0.0             0.0         0.0   \n",
       "3                0.0            0.0             0.0         0.0   \n",
       "4                0.0            0.0             0.0         0.0   \n",
       "\n",
       "   city_Adyar,Chennai  ...  cuisine_Thalis  cuisine_Tibetan  cuisine_Tribal  \\\n",
       "0                 0.0  ...             0.0              0.0             0.0   \n",
       "1                 0.0  ...             0.0              0.0             0.0   \n",
       "2                 0.0  ...             0.0              0.0             0.0   \n",
       "3                 0.0  ...             0.0              0.0             0.0   \n",
       "4                 0.0  ...             0.0              0.0             0.0   \n",
       "\n",
       "   cuisine_Turkish  cuisine_Use Code JUMBO30 to avail  \\\n",
       "0              0.0                                0.0   \n",
       "1              0.0                                0.0   \n",
       "2              0.0                                0.0   \n",
       "3              0.0                                0.0   \n",
       "4              0.0                                0.0   \n",
       "\n",
       "   cuisine_Use code XPRESS121 to avail.  cuisine_Vietnamese  cuisine_Waffle  \\\n",
       "0                                   0.0                 0.0             0.0   \n",
       "1                                   0.0                 0.0             0.0   \n",
       "2                                   0.0                 0.0             0.0   \n",
       "3                                   0.0                 0.0             0.0   \n",
       "4                                   0.0                 0.0             0.0   \n",
       "\n",
       "   cuisine_indian  original_index  \n",
       "0             0.0               0  \n",
       "1             0.0               0  \n",
       "2             0.0               1  \n",
       "3             0.0               1  \n",
       "4             0.0               2  \n",
       "\n",
       "[5 rows x 951 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "# Step 1: Load cleaned dataset\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Step 2: Assign original index\n",
    "df['original_index'] = df.index\n",
    "\n",
    "# Step 3: Split and explode cuisine column\n",
    "df['cuisine'] = df['cuisine'].str.split(',')\n",
    "df_exploded = df.explode('cuisine')\n",
    "df_exploded['cuisine'] = df_exploded['cuisine'].str.strip()\n",
    "\n",
    "# Step 4: One-Hot Encode 'city' and 'cuisine'\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_array = encoder.fit_transform(df_exploded[['city', 'cuisine']])\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['city', 'cuisine']))\n",
    "\n",
    "# Step 5: Add numeric columns + original index\n",
    "numerical = df_exploded[['rating', 'rating_count', 'cost']].reset_index(drop=True)\n",
    "encoded_final = pd.concat([numerical, encoded_df], axis=1)\n",
    "encoded_final['original_index'] = df_exploded['original_index'].values\n",
    "\n",
    "# Step 6: Save outputs\n",
    "encoded_final.to_csv(\"encoded_data.csv\", index=False)\n",
    "\n",
    "with open(\"encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoder, f)\n",
    "\n",
    "# Step 7: Preview\n",
    "encoded_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7355816-7843-44e3-b4fc-8a7cd29b77b5",
   "metadata": {},
   "source": [
    "## Recommendation Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b98af9-359c-44fd-bd07-9259b427566f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>rating</th>\n",
       "      <th>cost</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deccan mandi house</td>\n",
       "      <td>Kadapa</td>\n",
       "      <td>Biryani,Arabian</td>\n",
       "      <td>3.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Deccan mandi house, Beside Bombay Function Hal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sai Moksh Family Kitchen</td>\n",
       "      <td>Bhimavaram</td>\n",
       "      <td>North Indian,Biryani</td>\n",
       "      <td>3.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Sai Moksh Family Kitchen, D.No. 27-1-2/1 DNR C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kapil Restaurent</td>\n",
       "      <td>Beawar</td>\n",
       "      <td>Chinese,South Indian</td>\n",
       "      <td>3.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Kapil Restaurent, HOTEL SHANKAR PALACE AJMER R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Biryani Life</td>\n",
       "      <td>Lingampally &amp; Nalagandla,Hyderabad</td>\n",
       "      <td>Biryani,North Indian</td>\n",
       "      <td>3.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>The Biryani Life, First floor Sushma Arcade pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BLR Food Kourt</td>\n",
       "      <td>Tarnaka, Nacharam &amp; Malkajigiri,Hyderabad</td>\n",
       "      <td>Biryani,Chinese</td>\n",
       "      <td>3.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>BLR Food Kourt, blr food court,highlights home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>King Of Kabab</td>\n",
       "      <td>Valsad</td>\n",
       "      <td>Biryani,Mughlai</td>\n",
       "      <td>3.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>King Of Kabab, kosamba road opp jinnat nagar n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B13 Food Court</td>\n",
       "      <td>Tohana</td>\n",
       "      <td>North Indian,South Indian</td>\n",
       "      <td>3.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>B13 Food Court, NEAR SAINI CHOWK , RAILWAY ROA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Saffron9 Mandi House A Multi Cuisine Family Re...</td>\n",
       "      <td>Ramagundam</td>\n",
       "      <td>Biryani,South Indian</td>\n",
       "      <td>3.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Saffron9 Mandi House A Multi Cuisine Family Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Saffron9 Mandi House A Multi Cuisine Family Re...</td>\n",
       "      <td>Ramagundam</td>\n",
       "      <td>Biryani,South Indian</td>\n",
       "      <td>3.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Saffron9 Mandi House A Multi Cuisine Family Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Forex Haveli Restaurant</td>\n",
       "      <td>Allahabad</td>\n",
       "      <td>Indian,Biryani</td>\n",
       "      <td>3.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Forex Haveli Restaurant, Rehana Khatoon, 117, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                                 Deccan mandi house   \n",
       "1                           Sai Moksh Family Kitchen   \n",
       "2                                   Kapil Restaurent   \n",
       "3                                   The Biryani Life   \n",
       "4                                     BLR Food Kourt   \n",
       "5                                      King Of Kabab   \n",
       "6                                     B13 Food Court   \n",
       "7  Saffron9 Mandi House A Multi Cuisine Family Re...   \n",
       "8  Saffron9 Mandi House A Multi Cuisine Family Re...   \n",
       "9                            Forex Haveli Restaurant   \n",
       "\n",
       "                                        city                    cuisine  \\\n",
       "0                                     Kadapa            Biryani,Arabian   \n",
       "1                                 Bhimavaram       North Indian,Biryani   \n",
       "2                                     Beawar       Chinese,South Indian   \n",
       "3         Lingampally & Nalagandla,Hyderabad       Biryani,North Indian   \n",
       "4  Tarnaka, Nacharam & Malkajigiri,Hyderabad            Biryani,Chinese   \n",
       "5                                     Valsad            Biryani,Mughlai   \n",
       "6                                     Tohana  North Indian,South Indian   \n",
       "7                                 Ramagundam       Biryani,South Indian   \n",
       "8                                 Ramagundam       Biryani,South Indian   \n",
       "9                                  Allahabad             Indian,Biryani   \n",
       "\n",
       "   rating   cost                                            address  \n",
       "0     3.5  250.0  Deccan mandi house, Beside Bombay Function Hal...  \n",
       "1     3.5  250.0  Sai Moksh Family Kitchen, D.No. 27-1-2/1 DNR C...  \n",
       "2     3.5  250.0  Kapil Restaurent, HOTEL SHANKAR PALACE AJMER R...  \n",
       "3     3.5  250.0  The Biryani Life, First floor Sushma Arcade pl...  \n",
       "4     3.5  250.0  BLR Food Kourt, blr food court,highlights home...  \n",
       "5     3.5  250.0  King Of Kabab, kosamba road opp jinnat nagar n...  \n",
       "6     3.5  250.0  B13 Food Court, NEAR SAINI CHOWK , RAILWAY ROA...  \n",
       "7     3.5  250.0  Saffron9 Mandi House A Multi Cuisine Family Re...  \n",
       "8     3.5  250.0  Saffron9 Mandi House A Multi Cuisine Family Re...  \n",
       "9     3.5  250.0  Forex Haveli Restaurant, Rehana Khatoon, 117, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "# Step 1: Load data and encoder\n",
    "cleaned_df = pd.read_csv(\"cleaned_data.csv\")\n",
    "encoded_df = pd.read_csv(\"encoded_data.csv\")\n",
    "\n",
    "with open(\"encoder.pkl\", \"rb\") as f:\n",
    "    encoder = pickle.load(f)\n",
    "\n",
    "# Step 2: User input\n",
    "user_input = {\n",
    "    \"city\": \"Chennai\",\n",
    "    \"cuisine\": [\"South Indian\", \"Biryani\"],\n",
    "    \"rating\": 4.2,\n",
    "    \"rating_count\": 60,\n",
    "    \"cost\": 300\n",
    "}\n",
    "\n",
    "# Step 3: Prepare user input DataFrame\n",
    "input_df = pd.DataFrame([user_input])\n",
    "input_df = input_df.explode(\"cuisine\")\n",
    "input_df['cuisine'] = input_df['cuisine'].str.strip()\n",
    "\n",
    "# Step 4: One-hot encode user input\n",
    "encoded_input = encoder.transform(input_df[['city', 'cuisine']])\n",
    "encoded_input_df = pd.DataFrame(encoded_input, columns=encoder.get_feature_names_out(['city', 'cuisine']))\n",
    "\n",
    "# Step 5: Add numeric values\n",
    "numerical = pd.DataFrame({\n",
    "    'rating': [user_input['rating']] * len(input_df),\n",
    "    'rating_count': [user_input['rating_count']] * len(input_df),\n",
    "    'cost': [user_input['cost']] * len(input_df)\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "user_encoded = pd.concat([numerical, encoded_input_df], axis=1)\n",
    "\n",
    "# Step 6: Average across cuisines\n",
    "user_vector = user_encoded.mean().values.reshape(1, -1)\n",
    "\n",
    "# Step 7: Compute cosine similarity (drop 'original_index' for comparison)\n",
    "similarity_scores = cosine_similarity(user_vector, encoded_df.drop(columns=['original_index']))[0]\n",
    "\n",
    "# Step 8: Top 10 indices\n",
    "top_indices = similarity_scores.argsort()[-10:][::-1]\n",
    "\n",
    "# Step 9: Use original_index to map back to cleaned_df\n",
    "original_indices = encoded_df.iloc[top_indices]['original_index'].astype(int)\n",
    "recommendations = cleaned_df.iloc[original_indices][['name', 'city', 'cuisine', 'rating', 'cost', 'address']]\n",
    "recommendations.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 10: Show results\n",
    "recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187aaa5-a728-4427-af5b-7d3e18c8d3ec",
   "metadata": {},
   "source": [
    "## Data into MySQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de456b61-1f76-4b96-a548-91a95bedc5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database 'swiggy_reco' created.\n",
      "‚úÖ Column names cleaned, no duplicates.\n",
      "‚úÖ Old tables dropped if any.\n",
      "‚úÖ Inserted into 'restaurants_raw'\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Connect and Create DB\n",
    "# -------------------------------\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Dark2020@\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS swiggy_reco\")\n",
    "conn.close()\n",
    "print(\"‚úÖ Database 'swiggy_reco' created.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Load Data\n",
    "# -------------------------------\n",
    "df_raw = pd.read_csv(\"cleaned_data.csv\")\n",
    "df_encoded = pd.read_csv(\"encoded_data.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Clean Column Names\n",
    "# -------------------------------\n",
    "df_encoded.columns = df_encoded.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Remove duplicated columns explicitly\n",
    "df_encoded = df_encoded.loc[:, ~df_encoded.columns.duplicated()]\n",
    "\n",
    "# Check for duplicates after cleaning\n",
    "dupes = df_encoded.columns[df_encoded.columns.duplicated()].tolist()\n",
    "if dupes:\n",
    "    print(\"‚ùå Duplicate columns after cleaning:\", dupes)\n",
    "else:\n",
    "    print(\"‚úÖ Column names cleaned, no duplicates.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Upload to MySQL\n",
    "# -------------------------------\n",
    "engine = create_engine(\"mysql+mysqlconnector://root:Dark2020%40@localhost/swiggy_reco\")\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(\"DROP TABLE IF EXISTS restaurants_encoded\"))\n",
    "    connection.execute(text(\"DROP TABLE IF EXISTS restaurants_raw\"))\n",
    "    print(\"‚úÖ Old tables dropped if any.\")\n",
    "\n",
    "df_raw.to_sql(\"restaurants_raw\", con=engine, if_exists=\"replace\", index=False)\n",
    "print(\"‚úÖ Inserted into 'restaurants_raw'\")\n",
    "\n",
    "df_encoded.to_sql(\"restaurants_encoded\", con=engine, if_exists=\"replace\", index=False)\n",
    "print(\"‚úÖ Inserted into 'restaurants_encoded'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e16d87c-6133-4738-b4d4-d47fe80f3b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
      "/var/folders/sp/lgf9tmvs7z36z2vt_yc_ppcm0000gn/T/ipykernel_9354/332883041.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ cleaned_data.csv regenerated with one-hot cuisine columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load existing cleaned CSV\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Fill missing values\n",
    "df['cuisine'] = df['cuisine'].fillna(\"None\")\n",
    "\n",
    "# If cuisine is a comma-separated string like \"Indian, Chinese\", split it into list\n",
    "df['cuisine'] = df['cuisine'].apply(lambda x: [c.strip() for c in str(x).split(',')])\n",
    "\n",
    "# One-hot encode cuisine\n",
    "df_encoded = df.explode('cuisine')\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['cuisine'], prefix='cuisine')\n",
    "\n",
    "# Combine back (group by ID if available, else use other columns)\n",
    "df_encoded = df_encoded.groupby(df_encoded.columns.difference(['name']).tolist(), as_index=False).sum()\n",
    "\n",
    "# Save to new cleaned file\n",
    "df_encoded.to_csv(\"cleaned_data2.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ cleaned_data.csv regenerated with one-hot cuisine columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57399af5-a3c8-4574-9582-4c75a5556cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaned Data Columns: ['id', 'name', 'city', 'rating', 'rating_count', 'cost', 'cuisine', 'address']\n",
      "üß¨ Encoded Data Columns: ['address', 'city', 'cost', 'cuisine_8:15 To 11:30 Pm', 'cuisine_Afghani', 'cuisine_African', 'cuisine_American', 'cuisine_Andhra', 'cuisine_Arabian', 'cuisine_Asian', 'cuisine_Assamese', 'cuisine_Attractive Combos Available', 'cuisine_Australian', 'cuisine_Awadhi', 'cuisine_BEVERAGE', 'cuisine_Bakery', 'cuisine_Bakery products', 'cuisine_Bangladeshi', 'cuisine_Barbecue', 'cuisine_Bengali', 'cuisine_Beverages', 'cuisine_Bhutanese', 'cuisine_Bihari', 'cuisine_Biryani', 'cuisine_Biryani - Shivaji Military Hotel', 'cuisine_Bowl Company', 'cuisine_British', 'cuisine_Burgers', 'cuisine_Burmese', 'cuisine_Cafe', 'cuisine_Chaat', 'cuisine_Chettinad', 'cuisine_Chinese', 'cuisine_Coastal', 'cuisine_Code valid on bill over Rs.99', 'cuisine_Combo', 'cuisine_Continental', 'cuisine_Default', 'cuisine_Desserts', 'cuisine_Discount offer from Garden Cafe Express Kankurgachi', 'cuisine_European', 'cuisine_Fast Food', 'cuisine_Free Delivery ! Limited Stocks!', 'cuisine_French', 'cuisine_German', 'cuisine_Goan', 'cuisine_Greek', 'cuisine_Grill', 'cuisine_Grocery products', 'cuisine_Gujarati', 'cuisine_Haleem', 'cuisine_Healthy Food', 'cuisine_Home Food', 'cuisine_Hyderabadi', 'cuisine_Ice Cream', 'cuisine_Ice Cream Cakes', 'cuisine_Indian', 'cuisine_Indonesian', 'cuisine_Italian', 'cuisine_Italian-American', 'cuisine_Jain', 'cuisine_Japanese', 'cuisine_Juices', 'cuisine_Kashmiri', 'cuisine_Kebabs', 'cuisine_Kerala', 'cuisine_Keto', 'cuisine_Khasi', 'cuisine_Konkan', 'cuisine_Korean', 'cuisine_Lebanese', 'cuisine_Lucknowi', 'cuisine_MAX 2 Combos per Order!', 'cuisine_Maharashtrian', 'cuisine_Malaysian', 'cuisine_Malwani', 'cuisine_Mangalorean', 'cuisine_Meat', 'cuisine_Mediterranean', 'cuisine_Mexican', 'cuisine_Middle Eastern', 'cuisine_Mongolian', 'cuisine_Mughlai', 'cuisine_Naga', 'cuisine_Navratri Special', 'cuisine_Nepalese', 'cuisine_North Eastern', 'cuisine_North Indian', 'cuisine_Oriental', 'cuisine_Oriya', 'cuisine_Paan', 'cuisine_Pan-Asian', 'cuisine_Parsi', 'cuisine_Pastas', 'cuisine_Persian', 'cuisine_Pizzas', 'cuisine_Popular Brand Store', 'cuisine_Portuguese', 'cuisine_Punjabi', 'cuisine_Rajasthani', 'cuisine_Rayalaseema', 'cuisine_SVANidhi Street Food Vendor', 'cuisine_Salads', 'cuisine_Seafood', 'cuisine_Sindhi', 'cuisine_Singaporean', 'cuisine_Snacks', 'cuisine_South American', 'cuisine_South Indian', 'cuisine_Spanish', 'cuisine_Special Discount from (Hotel Swagath)', 'cuisine_Sri Lankan', 'cuisine_Steakhouse', 'cuisine_Street Food', 'cuisine_Sushi', 'cuisine_Sweets', 'cuisine_Tandoor', 'cuisine_Telangana', 'cuisine_Tex-Mex', 'cuisine_Thai', 'cuisine_Thalis', 'cuisine_Tibetan', 'cuisine_Tribal', 'cuisine_Turkish', 'cuisine_Use Code JUMBO30 to avail', 'cuisine_Use code XPRESS121 to avail.', 'cuisine_Vietnamese', 'cuisine_Waffle', 'cuisine_indian', 'id', 'rating', 'rating_count', 'name']\n",
      "üß¨ cleaned_data2 Columns: ['address', 'city', 'cost', 'cuisine_8:15 To 11:30 Pm', 'cuisine_Afghani', 'cuisine_African', 'cuisine_American', 'cuisine_Andhra', 'cuisine_Arabian', 'cuisine_Asian', 'cuisine_Assamese', 'cuisine_Attractive Combos Available', 'cuisine_Australian', 'cuisine_Awadhi', 'cuisine_BEVERAGE', 'cuisine_Bakery', 'cuisine_Bakery products', 'cuisine_Bangladeshi', 'cuisine_Barbecue', 'cuisine_Bengali', 'cuisine_Beverages', 'cuisine_Bhutanese', 'cuisine_Bihari', 'cuisine_Biryani', 'cuisine_Biryani - Shivaji Military Hotel', 'cuisine_Bowl Company', 'cuisine_British', 'cuisine_Burgers', 'cuisine_Burmese', 'cuisine_Cafe', 'cuisine_Chaat', 'cuisine_Chettinad', 'cuisine_Chinese', 'cuisine_Coastal', 'cuisine_Code valid on bill over Rs.99', 'cuisine_Combo', 'cuisine_Continental', 'cuisine_Default', 'cuisine_Desserts', 'cuisine_Discount offer from Garden Cafe Express Kankurgachi', 'cuisine_European', 'cuisine_Fast Food', 'cuisine_Free Delivery ! Limited Stocks!', 'cuisine_French', 'cuisine_German', 'cuisine_Goan', 'cuisine_Greek', 'cuisine_Grill', 'cuisine_Grocery products', 'cuisine_Gujarati', 'cuisine_Haleem', 'cuisine_Healthy Food', 'cuisine_Home Food', 'cuisine_Hyderabadi', 'cuisine_Ice Cream', 'cuisine_Ice Cream Cakes', 'cuisine_Indian', 'cuisine_Indonesian', 'cuisine_Italian', 'cuisine_Italian-American', 'cuisine_Jain', 'cuisine_Japanese', 'cuisine_Juices', 'cuisine_Kashmiri', 'cuisine_Kebabs', 'cuisine_Kerala', 'cuisine_Keto', 'cuisine_Khasi', 'cuisine_Konkan', 'cuisine_Korean', 'cuisine_Lebanese', 'cuisine_Lucknowi', 'cuisine_MAX 2 Combos per Order!', 'cuisine_Maharashtrian', 'cuisine_Malaysian', 'cuisine_Malwani', 'cuisine_Mangalorean', 'cuisine_Meat', 'cuisine_Mediterranean', 'cuisine_Mexican', 'cuisine_Middle Eastern', 'cuisine_Mongolian', 'cuisine_Mughlai', 'cuisine_Naga', 'cuisine_Navratri Special', 'cuisine_Nepalese', 'cuisine_North Eastern', 'cuisine_North Indian', 'cuisine_Oriental', 'cuisine_Oriya', 'cuisine_Paan', 'cuisine_Pan-Asian', 'cuisine_Parsi', 'cuisine_Pastas', 'cuisine_Persian', 'cuisine_Pizzas', 'cuisine_Popular Brand Store', 'cuisine_Portuguese', 'cuisine_Punjabi', 'cuisine_Rajasthani', 'cuisine_Rayalaseema', 'cuisine_SVANidhi Street Food Vendor', 'cuisine_Salads', 'cuisine_Seafood', 'cuisine_Sindhi', 'cuisine_Singaporean', 'cuisine_Snacks', 'cuisine_South American', 'cuisine_South Indian', 'cuisine_Spanish', 'cuisine_Special Discount from (Hotel Swagath)', 'cuisine_Sri Lankan', 'cuisine_Steakhouse', 'cuisine_Street Food', 'cuisine_Sushi', 'cuisine_Sweets', 'cuisine_Tandoor', 'cuisine_Telangana', 'cuisine_Tex-Mex', 'cuisine_Thai', 'cuisine_Thalis', 'cuisine_Tibetan', 'cuisine_Tribal', 'cuisine_Turkish', 'cuisine_Use Code JUMBO30 to avail', 'cuisine_Use code XPRESS121 to avail.', 'cuisine_Vietnamese', 'cuisine_Waffle', 'cuisine_indian', 'id', 'rating', 'rating_count', 'name']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_cleaned = pd.read_csv(\"cleaned_data.csv\")\n",
    "df_encoded = pd.read_csv(\"encoded_data.csv\")\n",
    "df_encoded = pd.read_csv(\"cleaned_data2.csv\")\n",
    "\n",
    "print(\"üßπ Cleaned Data Columns:\", df_cleaned.columns.tolist())\n",
    "print(\"üß¨ Encoded Data Columns:\", df_encoded.columns.tolist())\n",
    "print(\"üß¨ cleaned_data2 Columns:\", df_encoded.columns.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
